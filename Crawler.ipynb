{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Crawler.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1hZlMXLIqWPpVh3XG7L-lnNHp4yKnQX4n",
      "authorship_tag": "ABX9TyPfR7zklXWBC/RhlnE7/S6C"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufh043Unfpza",
        "outputId": "1a4d882a-f693-4ccc-d438-865b3ef8c3bc"
      },
      "source": [
        "!pip install beautifulsoup4\n",
        "!pip install numpy\n",
        "!pip install requests\n",
        "!pip install spacy\n",
        "!pip install trafilatura"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.6.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.0.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.7.4.3)\n",
            "Collecting trafilatura\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/eb/3725c95c55cf4f40c842f393e4310d569ca7efa7066f8ca7bcda757e27c7/trafilatura-0.8.2-py3-none-any.whl (165kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 6.4MB/s \n",
            "\u001b[?25hCollecting urllib3<2,>=1.25\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0c/cd/1e2ec680ec7b09846dc6e605f5a7709dfb9d7128e51a026e7154e18a234e/urllib3-1.26.5-py2.py3-none-any.whl (138kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 8.7MB/s \n",
            "\u001b[?25hCollecting lxml>=4.6.2; python_version > \"3.4\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/c0/d0526314971fc661b083ab135747dc68446a3022686da8c16d25fcf6ef07/lxml-4.6.3-cp37-cp37m-manylinux2014_x86_64.whl (6.3MB)\n",
            "\u001b[K     |████████████████████████████████| 6.3MB 7.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet>=3.0.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from trafilatura) (3.0.4)\n",
            "Collecting readability-lxml>=0.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/39/a6/cfe22aaa19ac69b97d127043a76a5bbcb0ef24f3a0b22793c46608190caa/readability_lxml-0.8.1-py3-none-any.whl\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from trafilatura) (2020.12.5)\n",
            "Collecting htmldate>=0.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/16/13/9284824d9269a357b2467d1274e4ecebc6bd80e1f90c033f6a77779703fa/htmldate-0.8.1-py3-none-any.whl\n",
            "Collecting courlan>=0.3.1\n",
            "  Downloading https://files.pythonhosted.org/packages/e5/0c/cedc2592fdd601b9cb9c26a4c533ab1a505ef3a3fdd7fb9ef21d84ca6633/courlan-0.4.0-py3-none-any.whl\n",
            "Collecting justext>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/5f/c7b909b4b864ebcacfac23ce2f6f01a50c53628787cc14b3c06f79464cab/jusText-2.2.0-py2.py3-none-any.whl (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 42.5MB/s \n",
            "\u001b[?25hCollecting cssselect\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
            "Collecting regex>=2020.11.13\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c4/28/5f08d8841013ccf72cd95dfff2500fe7fb39467af12c5e7b802d8381d811/regex-2021.4.4-cp37-cp37m-manylinux2014_x86_64.whl (720kB)\n",
            "\u001b[K     |████████████████████████████████| 727kB 37.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.7/dist-packages (from htmldate>=0.8.1->trafilatura) (2.8.1)\n",
            "Collecting dateparser>=1.0.0; python_version > \"3.4\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/78/c4/b5ddc3eeac974d85055d88c1e6b62cc492fc1a93dbe3b66a45a756a7b807/dateparser-1.0.0-py2.py3-none-any.whl (279kB)\n",
            "\u001b[K     |████████████████████████████████| 286kB 36.9MB/s \n",
            "\u001b[?25hCollecting tld; python_version >= \"3.6\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/51/ec8741d354a59450327be40591ef50b0ddb78bfb359fe1319003b233e5c8/tld-0.12.5-py37-none-any.whl (408kB)\n",
            "\u001b[K     |████████████████████████████████| 409kB 38.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.8.1->htmldate>=0.8.1->trafilatura) (1.15.0)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from dateparser>=1.0.0; python_version > \"3.4\"->htmldate>=0.8.1->trafilatura) (1.5.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from dateparser>=1.0.0; python_version > \"3.4\"->htmldate>=0.8.1->trafilatura) (2018.9)\n",
            "\u001b[31mERROR: requests 2.23.0 has requirement urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you'll have urllib3 1.26.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: urllib3, lxml, cssselect, readability-lxml, regex, dateparser, htmldate, tld, courlan, justext, trafilatura\n",
            "  Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Found existing installation: lxml 4.2.6\n",
            "    Uninstalling lxml-4.2.6:\n",
            "      Successfully uninstalled lxml-4.2.6\n",
            "  Found existing installation: regex 2019.12.20\n",
            "    Uninstalling regex-2019.12.20:\n",
            "      Successfully uninstalled regex-2019.12.20\n",
            "Successfully installed courlan-0.4.0 cssselect-1.1.0 dateparser-1.0.0 htmldate-0.8.1 justext-2.2.0 lxml-4.6.3 readability-lxml-0.8.1 regex-2021.4.4 tld-0.12.5 trafilatura-0.8.2 urllib3-1.26.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0WGtY3_hmrH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "311d804c-9b0c-4cf4-c9ce-347f35e9a538"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "import requests\n",
        "from requests.models import MissingSchema\n",
        "import spacy\n",
        "import trafilatura\n",
        "import time"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.5) or chardet (3.0.4) doesn't match a supported version!\n",
            "  RequestsDependencyWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YffX13VCwdTd"
      },
      "source": [
        "## **Extracting Text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veavwdhygBwT"
      },
      "source": [
        "def beautifulsoup_extract_text_fallback(response_content):\n",
        "    \n",
        "    # Create the beautifulsoup object:\n",
        "    soup = BeautifulSoup(response_content, 'html.parser')\n",
        "    \n",
        "    # Finding the text:\n",
        "    text = soup.find_all(text=True)\n",
        "    \n",
        "    # Remove unwanted tag elements:\n",
        "    cleaned_text = ''\n",
        "    blacklist = [\n",
        "        '[document]',\n",
        "        'noscript',\n",
        "        'header',\n",
        "        'html',\n",
        "        'meta',\n",
        "        'head', \n",
        "        'input',\n",
        "        'script',\n",
        "        'style',]\n",
        "\n",
        "    # Then we will loop over every item in the extract text and make sure that the beautifulsoup4 tag\n",
        "    # is NOT in the blacklist\n",
        "    for item in text:\n",
        "        if item.parent.name not in blacklist:\n",
        "            cleaned_text += '{} '.format(item)\n",
        "            \n",
        "    # Remove any tab separation and strip the text:\n",
        "    cleaned_text = cleaned_text.replace('\\t', '')\n",
        "    return cleaned_text.strip()\n",
        "    \n",
        "\n",
        "def extract_text_from_single_web_page(url):\n",
        "    \n",
        "    downloaded_url = trafilatura.fetch_url(url)\n",
        "    \n",
        "    try:\n",
        "        a = trafilatura.extract(downloaded_url, output_format='json', with_metadata=False, include_comments = False, include_images = False,                  \n",
        "              include_tables = False, include_links = False , date_extraction_params={'extensive_search': True, 'original_date': True})\n",
        "        \n",
        "    except AttributeError:\n",
        "        a = trafilatura.extract(downloaded_url, output_format='json', with_metadata=False, include_comments = False, include_images = False,            \n",
        "                    include_tables = False, include_links = False, date_extraction_params={'extensive_search': True, 'original_date': True})\n",
        "    \n",
        "    if a:\n",
        "        json_output = json.loads(a)\n",
        "        return json_output['text']\n",
        "    else:\n",
        "        try:\n",
        "            resp = requests.get(url)\n",
        "            # We will only extract the text from successful requests:\n",
        "            if resp.status_code == 200:\n",
        "                return beautifulsoup_extract_text_fallback(resp.content)\n",
        "            else:\n",
        "                # This line will handle for any failures in both the Trafilature and BeautifulSoup4 functions:\n",
        "                print('None')\n",
        "                return np.nan\n",
        "        # Handling for any URLs that don't have the correct protocol\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(e)\n",
        "            return np.nan\n",
        "        "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_pHDfGwQWrF"
      },
      "source": [
        "## **CSV Column Dropping**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8guJxLkQe53"
      },
      "source": [
        "real_path = r'/content/drive/MyDrive/Try CSV/Real'\n",
        "edited_path = r'/content/drive/MyDrive/Try CSV/Edited/'\n",
        "all_files = glob.glob(real_path + \"/*.csv\")\n",
        "#all_files\n",
        "\n",
        "columnsToDelete = [\"User Name\", \"Facebook Id\", \"Page Created\", \"Likes at Posting\", \"Followers at Posting\", \"Total Interactions\", \"Likes\", \"Comments\", \"Shares\", \"Love\", \"Wow\",\n",
        "                   \"Haha\", \"Sad\", \"Angry\", \"Care\", \"Is Video Owner?\", \"Post Views\", \"Total Views\", \"Total Views For All Crossposts\", \"Video Length\",\n",
        "                   \"Sponsor Id\", \"Sponsor Name\", \"Sponsor Category\", \"Total Interactions (weighted  —  Likes 1x Shares 1x Comments 1x Love 1x Wow 1x Haha 1x Sad 1x Angry 1x Care 1x )\",\n",
        "                   \"Overperforming Score\" ]\n",
        "\n",
        "# get all file names\n",
        "file_name=[]\n",
        "for files in all_files:\n",
        "  file_w_ext = (os.path.basename(files))\n",
        "  fname, ext = os.path.splitext(file_w_ext)\n",
        "  file_name.append(fname)\n",
        "\n",
        "#file_name\n",
        "\n",
        "# drop columns and create new 'Label' column\n",
        "for i in range(0, len(all_files)):\n",
        "  csv_file = pd.read_csv(all_files[i],encoding='utf-8')\n",
        "  dropped_file = csv_file.drop(columnsToDelete, axis=1, inplace=False)\n",
        "  dropped_file['Label'] = np.nan \n",
        "  dropped_file.to_csv(edited_path + file_name[i] + '_edited.csv', index=False, encoding='utf-8')\n",
        "  print(edited_path + file_name[i] + '_edited.csv created')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtgN1941w6Zn"
      },
      "source": [
        "## **Writing Description in  CSV**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ffilb1U-Xlqu"
      },
      "source": [
        "filePath = '/content/drive/MyDrive/Try CSV/Edited/' \n",
        "editedPath = '/content/drive/MyDrive/Try CSV/Crawled/'\n",
        "\n",
        "all_files = glob.glob(filePath + \"/*.csv\")\n",
        "#all_files\n",
        "\n",
        "# get all file names\n",
        "file_name=[]\n",
        "for files in all_files:\n",
        "  file_w_ext = (os.path.basename(files))\n",
        "  fname, ext = os.path.splitext(file_w_ext)\n",
        "  file_name.append(fname)\n",
        "\n",
        "#file_name\n",
        "\n",
        "unreachableSites = ['http://cumillabarta.com', 'https://www.analysisbd.net', 'https://bit.ly', 'https://www.hasivalobashi.club', \n",
        "                    'https://www.bengalbreakingnews.com', 'https://dailymorning24.com', 'https://www.sangbad24x7.com/', \n",
        "                    'http://www.naturalhealthtips.us/', 'https://kalerdarpan24.com', 'https://notunalo.press/', \n",
        "                    'https://www.timeofkushtia.com/', 'https://somoybd24.info/', 'https://www.sarakhon.com/']\n",
        "\n",
        "for j in range(len(all_files)):\n",
        "  df = pd.read_csv(all_files[j],encoding='utf-8')\n",
        "  for i in range(len(df)):\n",
        "    print(i)\n",
        "\n",
        "    #if Status or Link is of Facebook, dont do anything\n",
        "    if df.loc[i, 'Type'] == 'Status' or 'https://www.facebook.com' in df.loc[i, 'Link'] :\n",
        "      print('Status Type')\n",
        "      print('Continuing')\n",
        "      continue\n",
        "      #URL = df.loc[i, 'URL']\n",
        "\n",
        "    elif df.loc[i, 'Type'] == 'Link':\n",
        "      print('Link Type')\n",
        "      URL = df.loc[i, 'Link']\n",
        "      # if 'https://www.facebook.com' in URL:\n",
        "      #   print('Facebook')\n",
        "      #   print('Continuing')\n",
        "      #   continue \n",
        "\n",
        "    else :\n",
        "      df.drop([i], inplace=True)\n",
        "      print('Other type')\n",
        "      print('dropped')\n",
        "      continue\n",
        "    \n",
        "    print(URL)\n",
        "\n",
        "    if any(x in URL for x in unreachableSites ):\n",
        "      df.drop([i], inplace = True )\n",
        "      print('dropped as unreachable')\n",
        "      continue\n",
        "\n",
        "\n",
        "    try:\n",
        "      text = extract_text_from_single_web_page(url=URL)\n",
        "    except KeyboardInterrupt as e:\n",
        "      print('Key pressed')\n",
        "      df.drop([i], inplace = True )\n",
        "      print('dropped as key pressed')\n",
        "      continue\n",
        "      \n",
        "    \n",
        "\n",
        "    #  if the url is unreachable, drop it\n",
        "    if text is np.nan:\n",
        "      df.drop([i], inplace = True )\n",
        "      print('dropped')\n",
        "      continue\n",
        "\n",
        "    # replace Description with text extracted from the link\n",
        "    df.replace(to_replace = df.loc[i, 'Description'], \n",
        "                 value = text, \n",
        "                  inplace = True)\n",
        "    print(\"Description Updated\")\n",
        "    #print(df.loc[i,'Description'])\n",
        "\n",
        "  df.to_csv(editedPath + file_name[j] +  'crawled.csv', index=False, encoding='utf-8')\n",
        "  print(editedPath + file_name[j] +  'crawled.csv created')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOiB7dLXIN_x"
      },
      "source": [
        "# **Testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8v_M8tfgqXj"
      },
      "source": [
        "single_url = 'https://www.analysisbd.net/archives/15992'\n",
        "\n",
        "text = extract_text_from_single_web_page(url=single_url)\n",
        "\n",
        "    \n",
        "print(text)\n",
        "if text is np.nan:\n",
        "  print('NAN')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}